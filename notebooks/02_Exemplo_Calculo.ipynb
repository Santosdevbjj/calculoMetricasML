# 02_Exemplo_Calculo.ipynb

# %% [markdown]
# ## Calculando Métricas de Avaliação de Aprendizado
#
# Este notebook demonstra como utilizar as funções desenvolvidas no módulo `metrics_calculator` para calcular métricas essenciais de avaliação de modelos de classificação.
#
# ### 1. Importando as Funções
#
# Primeiro, vamos importar as funções que criamos.

# %%
from src.metrics_calculator import (
    calcular_sensibilidade,
    calcular_especificidade,
    calcular_acuracia,
    calcular_precisao,
    calcular_fscore
)

# %% [markdown]
# ### 2. Definindo a Matriz de Confusão
#
# Vamos usar os mesmos valores arbitrários de antes para a matriz de confusão.

# %%
# Valores da Matriz de Confusão
vp = 80  # Verdadeiros Positivos
fn = 20  # Falsos Negativos
fp = 10  # Falsos Positivos
vn = 90  # Verdadeiros Negativos

print(f"Matriz de Confusão Utilizada:")
print(f"  VP: {vp}")
print(f"  FN: {fn}")
print(f"  FP: {fp}")
print(f"  VN: {vn}")

# %% [markdown]
# ### 3. Calculando e Exibindo as Métricas
#
# Agora, vamos aplicar as funções importadas para calcular cada métrica e exibir os resultados.

# %%
# Cálculo das Métricas
sensibilidade = calcular_sensibilidade(vp, fn)
especificidade = calcular_especificidade(vn, fp)
acuracia = calcular_acuracia(vp, fn, fp, vn)
precisao = calcular_precisao(vp, fp)
f1 = calcular_fscore(precisao, sensibilidade)

# Exibição dos Resultados
print("\n--- Resultados das Métricas ---")
print(f"Sensibilidade (Recall): {sensibilidade:.4f}")
print(f"Especificidade:         {especificidade:.4f}")
print(f"Acurácia:               {acuracia:.4f}")
print(f"Precisão:               {precisao:.4f}")
print(f"F-score:                {f1:.4f}")
print("\n------------------------------")

# %% [markdown]
# ### 4. Interpretando os Resultados
#
# Com os resultados em mãos, podemos começar a interpretar o desempenho do modelo:
#
# * **Sensibilidade (Recall):** Indica a proporção de casos positivos reais que foram corretamente identificados. Um valor alto é desejável quando é importante não perder casos positivos (ex: detecção de doenças).
# * **Especificidade:** Indica a proporção de casos negativos reais que foram corretamente identificados. Um valor alto é desejável quando é importante não classificar indevidamente casos negativos como positivos (ex: detecção de fraudes).
# * **Acurácia:** É a métrica mais geral, representando a proporção total de previsões corretas (positivas e negativas) em relação ao total de instâncias. Pode ser enganosa em datasets desbalanceados.
# * **Precisão:** Indica a proporção de previsões positivas que foram de fato corretas. Um valor alto é desejável quando o custo de um falso positivo é alto.
# * **F-score:** É a média harmônica da precisão e da sensibilidade. Fornece um equilíbrio entre essas duas métricas, sendo útil especialmente em datasets desbalanceados. Um F-score alto indica que o modelo tem boa precisão e boa sensibilidade.
